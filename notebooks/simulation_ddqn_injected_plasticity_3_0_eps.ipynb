{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:46:48.377979Z",
     "start_time": "2024-04-15T11:46:48.375819Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.main import train_dqn\n",
    "from src.visualization import plot_rewards, plot_q, plot_all_rewards"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashfaq/.local/share/virtualenvs/rl-atari-7yjpj432/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Steps: 567, eps: 1.000, total rewards: 310.0\n",
      "Episode: 2, Steps: 554, eps: 1.000, total rewards: 80.0\n",
      "Episode: 3, Steps: 405, eps: 0.999, total rewards: 90.0\n",
      "Episode: 4, Steps: 500, eps: 0.999, total rewards: 210.0\n",
      "Episode: 5, Steps: 395, eps: 0.999, total rewards: 75.0\n",
      "Episode: 6, Steps: 317, eps: 0.999, total rewards: 85.0\n",
      "Episode: 7, Steps: 577, eps: 0.998, total rewards: 110.0\n",
      "Episode: 8, Steps: 492, eps: 0.998, total rewards: 180.0\n",
      "Episode: 9, Steps: 359, eps: 0.998, total rewards: 135.0\n",
      "Episode: 10, Steps: 730, eps: 0.997, total rewards: 440.0\n",
      "Episode: 11, Steps: 881, eps: 0.997, total rewards: 300.0\n",
      "Episode: 12, Steps: 345, eps: 0.997, total rewards: 80.0\n",
      "Episode: 13, Steps: 883, eps: 0.997, total rewards: 345.0\n",
      "Episode: 14, Steps: 492, eps: 0.996, total rewards: 110.0\n",
      "Episode: 15, Steps: 442, eps: 0.996, total rewards: 135.0\n",
      "Episode: 16, Steps: 781, eps: 0.996, total rewards: 260.0\n",
      "Episode: 17, Steps: 532, eps: 0.995, total rewards: 130.0\n",
      "Episode: 18, Steps: 288, eps: 0.995, total rewards: 45.0\n",
      "Episode: 19, Steps: 522, eps: 0.995, total rewards: 140.0\n",
      "Episode: 20, Steps: 467, eps: 0.995, total rewards: 60.0\n",
      "Episode: 21, Steps: 618, eps: 0.994, total rewards: 160.0\n",
      "Episode: 22, Steps: 574, eps: 0.994, total rewards: 230.0\n",
      "Episode: 23, Steps: 515, eps: 0.994, total rewards: 185.0\n",
      "Episode: 24, Steps: 336, eps: 0.993, total rewards: 50.0\n",
      "Episode: 25, Steps: 435, eps: 0.993, total rewards: 65.0\n",
      "Episode: 26, Steps: 506, eps: 0.993, total rewards: 155.0\n",
      "Episode: 27, Steps: 532, eps: 0.993, total rewards: 180.0\n",
      "Episode: 28, Steps: 498, eps: 0.992, total rewards: 155.0\n",
      "Episode: 29, Steps: 477, eps: 0.992, total rewards: 120.0\n",
      "Episode: 30, Steps: 594, eps: 0.992, total rewards: 210.0\n",
      "Episode: 31, Steps: 378, eps: 0.991, total rewards: 90.0\n",
      "Episode: 32, Steps: 576, eps: 0.991, total rewards: 155.0\n",
      "Episode: 33, Steps: 272, eps: 0.991, total rewards: 80.0\n",
      "Episode: 34, Steps: 1038, eps: 0.991, total rewards: 585.0\n",
      "Episode: 35, Steps: 471, eps: 0.990, total rewards: 110.0\n",
      "Episode: 36, Steps: 579, eps: 0.990, total rewards: 215.0\n",
      "Episode: 37, Steps: 487, eps: 0.990, total rewards: 80.0\n",
      "Episode: 38, Steps: 306, eps: 0.989, total rewards: 50.0\n",
      "Episode: 39, Steps: 864, eps: 0.989, total rewards: 195.0\n",
      "Episode: 40, Steps: 605, eps: 0.989, total rewards: 155.0\n",
      "Episode: 41, Steps: 506, eps: 0.989, total rewards: 135.0\n",
      "Episode: 42, Steps: 654, eps: 0.988, total rewards: 150.0\n",
      "Episode: 43, Steps: 333, eps: 0.988, total rewards: 60.0\n",
      "Episode: 44, Steps: 843, eps: 0.988, total rewards: 240.0\n",
      "Episode: 45, Steps: 558, eps: 0.987, total rewards: 160.0\n",
      "Episode: 46, Steps: 293, eps: 0.987, total rewards: 65.0\n",
      "Episode: 47, Steps: 617, eps: 0.987, total rewards: 410.0\n",
      "Episode: 48, Steps: 262, eps: 0.987, total rewards: 55.0\n",
      "Episode: 49, Steps: 421, eps: 0.986, total rewards: 155.0\n",
      "Episode: 50, Steps: 331, eps: 0.986, total rewards: 80.0\n",
      "Episode: 51, Steps: 440, eps: 0.986, total rewards: 50.0\n",
      "Episode: 52, Steps: 730, eps: 0.985, total rewards: 225.0\n",
      "Episode: 53, Steps: 369, eps: 0.985, total rewards: 45.0\n",
      "Episode: 54, Steps: 399, eps: 0.985, total rewards: 35.0\n",
      "Episode: 55, Steps: 286, eps: 0.985, total rewards: 80.0\n",
      "Episode: 56, Steps: 728, eps: 0.984, total rewards: 245.0\n",
      "Episode: 57, Steps: 805, eps: 0.984, total rewards: 530.0\n",
      "Episode: 58, Steps: 941, eps: 0.984, total rewards: 555.0\n",
      "Episode: 59, Steps: 590, eps: 0.983, total rewards: 210.0\n",
      "Episode: 60, Steps: 883, eps: 0.983, total rewards: 395.0\n",
      "Episode: 61, Steps: 407, eps: 0.983, total rewards: 45.0\n",
      "Episode: 62, Steps: 596, eps: 0.983, total rewards: 120.0\n",
      "Episode: 63, Steps: 518, eps: 0.982, total rewards: 105.0\n",
      "Episode: 64, Steps: 479, eps: 0.982, total rewards: 120.0\n",
      "Episode: 65, Steps: 336, eps: 0.982, total rewards: 85.0\n",
      "Episode: 66, Steps: 489, eps: 0.981, total rewards: 125.0\n",
      "Episode: 67, Steps: 266, eps: 0.981, total rewards: 40.0\n",
      "Episode: 68, Steps: 512, eps: 0.981, total rewards: 110.0\n",
      "Episode: 69, Steps: 374, eps: 0.981, total rewards: 95.0\n",
      "Episode: 70, Steps: 650, eps: 0.980, total rewards: 240.0\n",
      "Episode: 71, Steps: 617, eps: 0.980, total rewards: 135.0\n",
      "Episode: 72, Steps: 285, eps: 0.980, total rewards: 35.0\n",
      "Episode: 73, Steps: 456, eps: 0.979, total rewards: 110.0\n",
      "Episode: 74, Steps: 297, eps: 0.979, total rewards: 5.0\n",
      "Episode: 75, Steps: 925, eps: 0.979, total rewards: 200.0\n",
      "Episode: 76, Steps: 438, eps: 0.979, total rewards: 135.0\n",
      "Episode: 77, Steps: 585, eps: 0.978, total rewards: 165.0\n",
      "Episode: 78, Steps: 591, eps: 0.978, total rewards: 180.0\n",
      "Episode: 79, Steps: 422, eps: 0.978, total rewards: 20.0\n",
      "Episode: 80, Steps: 457, eps: 0.977, total rewards: 105.0\n",
      "Episode: 81, Steps: 598, eps: 0.977, total rewards: 170.0\n",
      "Episode: 82, Steps: 491, eps: 0.977, total rewards: 135.0\n",
      "Episode: 83, Steps: 482, eps: 0.977, total rewards: 135.0\n",
      "Episode: 84, Steps: 718, eps: 0.976, total rewards: 210.0\n",
      "Episode: 85, Steps: 579, eps: 0.976, total rewards: 145.0\n",
      "Episode: 86, Steps: 387, eps: 0.976, total rewards: 105.0\n",
      "Episode: 87, Steps: 461, eps: 0.975, total rewards: 30.0\n",
      "Episode: 88, Steps: 459, eps: 0.975, total rewards: 80.0\n",
      "Episode: 89, Steps: 494, eps: 0.975, total rewards: 125.0\n",
      "Episode: 90, Steps: 835, eps: 0.975, total rewards: 485.0\n",
      "Episode: 91, Steps: 502, eps: 0.974, total rewards: 170.0\n",
      "Episode: 92, Steps: 515, eps: 0.974, total rewards: 155.0\n",
      "Episode: 93, Steps: 465, eps: 0.974, total rewards: 65.0\n",
      "Episode: 94, Steps: 520, eps: 0.973, total rewards: 95.0\n",
      "Episode: 95, Steps: 558, eps: 0.973, total rewards: 110.0\n",
      "Episode: 96, Steps: 893, eps: 0.973, total rewards: 230.0\n",
      "Episode: 97, Steps: 640, eps: 0.973, total rewards: 210.0\n",
      "Episode: 98, Steps: 290, eps: 0.972, total rewards: 55.0\n",
      "Episode: 99, Steps: 301, eps: 0.972, total rewards: 65.0\n",
      "Episode: 100, Steps: 565, eps: 0.972, total rewards: 180.0\n",
      "Episode: 101, Steps: 326, eps: 0.971, total rewards: 30.0\n",
      "Episode: 102, Steps: 424, eps: 0.971, total rewards: 80.0\n",
      "Episode: 103, Steps: 601, eps: 0.971, total rewards: 210.0\n",
      "Episode: 104, Steps: 452, eps: 0.971, total rewards: 80.0\n",
      "Episode: 105, Steps: 562, eps: 0.970, total rewards: 105.0\n",
      "Episode: 106, Steps: 437, eps: 0.970, total rewards: 130.0\n",
      "Episode: 107, Steps: 526, eps: 0.970, total rewards: 150.0\n",
      "Episode: 108, Steps: 306, eps: 0.969, total rewards: 50.0\n",
      "Episode: 109, Steps: 491, eps: 0.969, total rewards: 120.0\n",
      "Episode: 110, Steps: 734, eps: 0.969, total rewards: 330.0\n",
      "Episode: 111, Steps: 567, eps: 0.969, total rewards: 400.0\n",
      "Episode: 112, Steps: 479, eps: 0.968, total rewards: 60.0\n",
      "Episode: 113, Steps: 779, eps: 0.968, total rewards: 475.0\n",
      "Episode: 114, Steps: 383, eps: 0.968, total rewards: 35.0\n",
      "Episode: 115, Steps: 631, eps: 0.967, total rewards: 215.0\n",
      "Episode: 116, Steps: 460, eps: 0.967, total rewards: 45.0\n",
      "Episode: 117, Steps: 606, eps: 0.967, total rewards: 135.0\n",
      "Episode: 118, Steps: 559, eps: 0.967, total rewards: 120.0\n",
      "Episode: 119, Steps: 305, eps: 0.966, total rewards: 60.0\n",
      "Episode: 120, Steps: 317, eps: 0.966, total rewards: 55.0\n",
      "Episode: 121, Steps: 422, eps: 0.966, total rewards: 135.0\n",
      "Episode: 122, Steps: 838, eps: 0.965, total rewards: 275.0\n",
      "Episode: 123, Steps: 342, eps: 0.965, total rewards: 90.0\n",
      "Episode: 124, Steps: 884, eps: 0.965, total rewards: 465.0\n",
      "Episode: 125, Steps: 523, eps: 0.965, total rewards: 135.0\n",
      "Episode: 126, Steps: 506, eps: 0.964, total rewards: 170.0\n",
      "Episode: 127, Steps: 629, eps: 0.964, total rewards: 195.0\n",
      "Episode: 128, Steps: 306, eps: 0.964, total rewards: 100.0\n",
      "Episode: 129, Steps: 563, eps: 0.963, total rewards: 120.0\n",
      "Episode: 130, Steps: 478, eps: 0.963, total rewards: 135.0\n",
      "Episode: 131, Steps: 615, eps: 0.963, total rewards: 80.0\n",
      "Episode: 132, Steps: 581, eps: 0.963, total rewards: 155.0\n",
      "Episode: 133, Steps: 418, eps: 0.962, total rewards: 50.0\n",
      "Episode: 134, Steps: 581, eps: 0.962, total rewards: 135.0\n",
      "Episode: 135, Steps: 551, eps: 0.962, total rewards: 215.0\n",
      "Episode: 136, Steps: 903, eps: 0.961, total rewards: 455.0\n",
      "Episode: 137, Steps: 450, eps: 0.961, total rewards: 135.0\n",
      "Episode: 138, Steps: 288, eps: 0.961, total rewards: 105.0\n",
      "Episode: 139, Steps: 478, eps: 0.961, total rewards: 125.0\n",
      "Episode: 140, Steps: 484, eps: 0.960, total rewards: 45.0\n",
      "Episode: 141, Steps: 539, eps: 0.960, total rewards: 110.0\n",
      "Episode: 142, Steps: 613, eps: 0.960, total rewards: 130.0\n",
      "Episode: 143, Steps: 348, eps: 0.959, total rewards: 110.0\n",
      "Episode: 144, Steps: 594, eps: 0.959, total rewards: 170.0\n",
      "Episode: 145, Steps: 384, eps: 0.959, total rewards: 120.0\n",
      "Episode: 146, Steps: 703, eps: 0.959, total rewards: 135.0\n",
      "Episode: 147, Steps: 575, eps: 0.958, total rewards: 120.0\n",
      "Episode: 148, Steps: 514, eps: 0.958, total rewards: 180.0\n",
      "Episode: 149, Steps: 1046, eps: 0.958, total rewards: 560.0\n",
      "Episode: 150, Steps: 306, eps: 0.957, total rewards: 45.0\n",
      "Episode: 151, Steps: 546, eps: 0.957, total rewards: 120.0\n",
      "Episode: 152, Steps: 426, eps: 0.957, total rewards: 65.0\n",
      "Episode: 153, Steps: 683, eps: 0.957, total rewards: 410.0\n",
      "Episode: 154, Steps: 489, eps: 0.956, total rewards: 155.0\n",
      "Episode: 155, Steps: 445, eps: 0.956, total rewards: 150.0\n",
      "Episode: 156, Steps: 490, eps: 0.956, total rewards: 80.0\n",
      "Episode: 157, Steps: 562, eps: 0.955, total rewards: 210.0\n",
      "Episode: 158, Steps: 497, eps: 0.955, total rewards: 60.0\n",
      "Episode: 159, Steps: 479, eps: 0.955, total rewards: 120.0\n",
      "Episode: 160, Steps: 338, eps: 0.955, total rewards: 50.0\n",
      "Episode: 161, Steps: 411, eps: 0.954, total rewards: 30.0\n",
      "Episode: 162, Steps: 478, eps: 0.954, total rewards: 105.0\n",
      "Episode: 163, Steps: 420, eps: 0.954, total rewards: 80.0\n",
      "Episode: 164, Steps: 450, eps: 0.953, total rewards: 105.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m spaceinvaders_rewards, spaceinvaders_max_steps, spaceinvaders_q_values, spaceinvaders_model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_dqn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mALE/SpaceInvaders-v5\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mddqn_injected_plasticity\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m      4\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/rl-atari/src/main.py:84\u001B[0m, in \u001B[0;36mtrain_dqn\u001B[0;34m(env_name, method, n_episodes, n_steps, discount_factor, learning_rate, model_seed, env_seed, replay_buff_max_len, initial_training_percentage, eta, alpha)\u001B[0m\n\u001B[1;32m     68\u001B[0m     rewards_per_episode, steps_over_episode, q_values_over_episode \u001B[38;5;241m=\u001B[39m play_multiple_episodes_dqn(\n\u001B[1;32m     69\u001B[0m         env\u001B[38;5;241m=\u001B[39menv,\n\u001B[1;32m     70\u001B[0m         model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     81\u001B[0m         replay_buff_max_len\u001B[38;5;241m=\u001B[39mreplay_buff_max_len\n\u001B[1;32m     82\u001B[0m     )\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mddqn_injected_plasticity\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m---> 84\u001B[0m     rewards_per_episode, steps_over_episode, q_values_over_episode \u001B[38;5;241m=\u001B[39m \u001B[43mplay_multiple_episodes_dqn_plastic\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     85\u001B[0m \u001B[43m        \u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     86\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     87\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtarget_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_episodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     90\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction_count\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     91\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhistory_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mHISTORY_LEN\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdiscount_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdiscount_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     93\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     94\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     95\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     96\u001B[0m \u001B[43m        \u001B[49m\u001B[43mframe_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mFRAME_SHAPE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     97\u001B[0m \u001B[43m        \u001B[49m\u001B[43minitial_training_percentage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial_training_percentage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     98\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreplay_buff_max_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreplay_buff_max_len\u001B[49m\n\u001B[1;32m     99\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown method: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(method))\n",
      "File \u001B[0;32m~/PycharmProjects/rl-atari/src/agents/agent_dqn_injected_plasticity.py:26\u001B[0m, in \u001B[0;36mplay_multiple_episodes_dqn_plastic\u001B[0;34m(env, model, target_model, n_episodes, n_steps, n_outputs, history_len, discount_factor, batch_size, optimizer, loss_fn, frame_shape, initial_training_percentage, replay_buff_max_len)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplay_multiple_episodes_dqn_plastic\u001B[39m(\n\u001B[1;32m     11\u001B[0m         env,\n\u001B[1;32m     12\u001B[0m         model,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     24\u001B[0m         replay_buff_max_len\n\u001B[1;32m     25\u001B[0m ):\n\u001B[0;32m---> 26\u001B[0m     rewards_over_episodes, steps_over_episodes, avg_max_q_values \u001B[38;5;241m=\u001B[39m \u001B[43mplay_multiple_episodes_dqn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m        \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtarget_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mn_episodes\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43minitial_training_percentage\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhistory_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdiscount_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m        \u001B[49m\u001B[43mframe_shape\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreplay_buff_max_len\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m     model\u001B[38;5;241m.\u001B[39minject_plasticity()\n\u001B[1;32m     44\u001B[0m     rewards_over_episodes, steps_over_episodes, avg_max_q_values \u001B[38;5;241m=\u001B[39m play_multiple_episodes_dqn_inject_plasticity(\n\u001B[1;32m     45\u001B[0m         env,\n\u001B[1;32m     46\u001B[0m         model,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     60\u001B[0m         avg_max_q_values\n\u001B[1;32m     61\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/rl-atari/src/agents/dqn_agent.py:38\u001B[0m, in \u001B[0;36mplay_multiple_episodes_dqn\u001B[0;34m(env, model, target_model, n_episodes, n_steps, n_outputs, history_len, discount_factor, batch_size, optimizer, loss_fn, frame_shape, replay_buff_max_len)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m episode \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_episodes):\n\u001B[1;32m     36\u001B[0m     epsilon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m episode \u001B[38;5;241m/\u001B[39m n_episodes, \u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m---> 38\u001B[0m     episode_reward, max_step_of_episode \u001B[38;5;241m=\u001B[39m \u001B[43mplay_one_episode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m        \u001B[49m\u001B[43mepisode_idx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepisode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[43m        \u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[43m        \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepsilon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreplay_buffer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreplay_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhistory_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhistory_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[43m        \u001B[49m\u001B[43mframe_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mframe_shape\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m     rewards_over_episodes\u001B[38;5;241m.\u001B[39mappend(episode_reward)\n\u001B[1;32m     51\u001B[0m     steps_over_episodes\u001B[38;5;241m.\u001B[39mappend(max_step_of_episode)\n",
      "File \u001B[0;32m~/PycharmProjects/rl-atari/src/agents/dqn_agent.py:107\u001B[0m, in \u001B[0;36mplay_one_episode\u001B[0;34m(episode_idx, env, model, n_steps, n_outputs, epsilon, replay_buffer, history_len, frame_shape)\u001B[0m\n\u001B[1;32m    104\u001B[0m max_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_steps):\n\u001B[0;32m--> 107\u001B[0m     reward, done, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[43mplay_one_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[43m        \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_queue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    110\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    111\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreplay_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    113\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[43m        \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    115\u001B[0m \u001B[43m        \u001B[49m\u001B[43mframe_shape\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    118\u001B[0m     max_step \u001B[38;5;241m=\u001B[39m step\n\u001B[1;32m    119\u001B[0m     episode_rewards\u001B[38;5;241m.\u001B[39mappend(reward)\n",
      "File \u001B[0;32m~/PycharmProjects/rl-atari/src/agents/dqn_agent.py:82\u001B[0m, in \u001B[0;36mplay_one_step\u001B[0;34m(env, state_queue, model, n_outputs, replay_buffer, step_idx, epsilon, frame_shape)\u001B[0m\n\u001B[1;32m     80\u001B[0m state_history \u001B[38;5;241m=\u001B[39m state_queue\u001B[38;5;241m.\u001B[39mget_history()\n\u001B[1;32m     81\u001B[0m action \u001B[38;5;241m=\u001B[39m epsilon_greedy_policy(state_history, model, n_outputs, epsilon)\n\u001B[0;32m---> 82\u001B[0m next_state, reward, done, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     84\u001B[0m last_state \u001B[38;5;241m=\u001B[39m state_history[:, :, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, np\u001B[38;5;241m.\u001B[39mnewaxis]\n\u001B[1;32m     86\u001B[0m preprocessed_next_state \u001B[38;5;241m=\u001B[39m frame_processor(next_state, shape\u001B[38;5;241m=\u001B[39mframe_shape)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/rl-atari-7yjpj432/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/rl-atari-7yjpj432/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:51\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/rl-atari-7yjpj432/lib/python3.11/site-packages/shimmy/atari_env.py:294\u001B[0m, in \u001B[0;36mAtariEnv.step\u001B[0;34m(self, action_ind)\u001B[0m\n\u001B[1;32m    292\u001B[0m reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(frameskip):\n\u001B[0;32m--> 294\u001B[0m     reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43male\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    295\u001B[0m is_terminal \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39male\u001B[38;5;241m.\u001B[39mgame_over(with_truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    296\u001B[0m is_truncated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39male\u001B[38;5;241m.\u001B[39mgame_truncated()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "seaquest_rewards, seaquest_max_steps, seaquest_q_values, seaquest_model = train_dqn(\n",
    "    \"ALE/Seaquest-v5\", \n",
    "    method=\"ddqn_injected_plasticity\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:48:20.060147Z",
     "start_time": "2024-04-15T11:46:49.880125Z"
    }
   },
   "id": "c46b64e827b718ac",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_rewards(seaquest_rewards, title='SeaQuest Rewards Over Episodes')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df9d81a80add263f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_q(seaquest_q_values, title='SeaQuest Q Values Over Episodes')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9363c284e58742dd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "np.save(\"../data/save/seaquest_rewards_ddqn_injected_plasticity_0_eps.npy\", np.array(seaquest_rewards))\n",
    "np.save(\"../data/save/seaquest_max_steps_ddqn_injected_plasticity_0_eps.npy\", np.array(seaquest_max_steps))\n",
    "np.save(\"../data/save/seaquest_q_values_ddqn_injected_plasticity_0_eps.npy\", np.array(seaquest_q_values))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ac834abdda83c15"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "seaquest_model.save(\"../data/saved_models/seaquest_model_ddqn_injected_plasticity_0_eps.keras\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2dc338e9aaca9f0d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6ea3e38895176ae5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
